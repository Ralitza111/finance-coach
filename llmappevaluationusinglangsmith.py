# -*- coding: utf-8 -*-
"""LLMAppEvaluationUsingLangsmith.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ch_BEqRq0WMw9vb5Au1_Xh8KgkbQjKag
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# !pip install langchain langsmith openai datasets langchain_openai

import os, getpass
os.environ["OPENAI_API_KEY"] = getpass.getpass("ðŸ”‘ Enter your OpenAI API Key: ")

os.environ["LANGCHAIN_API_KEY"] = getpass.getpass("ðŸ”‘ Enter your LangSmith API Key: ")

# Optional Hugging Face token (only if dataset requires it)
try:
    hf_token = getpass.getpass("ðŸ”‘ Enter your Hugging Face Token (or press Enter to skip): ")
    if hf_token.strip():
        os.environ["HF_TOKEN"] = hf_token
except Exception:
    pass

# LangSmith project setup
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "colab-langsmith-eval-demo"

from langsmith import Client
client = Client()

# # Load a Hugging Face Dataset
# from datasets import load_dataset

# # Use TriviaQA (20 samples for demo)
# if "HF_TOKEN" in os.environ:
#     dataset = load_dataset("trivia_qa", "unfiltered", split="validation[:20]", token=os.environ["HF_TOKEN"])
# else:
#     dataset = load_dataset("trivia_qa", "unfiltered", split="validation[:20]")

# print("Sample entry:", dataset[0])

from datasets import load_dataset
from itertools import islice

# Stream only, no full download
dataset_stream = load_dataset("trivia_qa", "unfiltered", split="validation", streaming=True)

# Take first 20 samples from the stream
dataset = list(islice(dataset_stream, 20))

print("âœ… Number of samples loaded:", len(dataset))
print("ðŸ“˜ Sample entry:\n", dataset[0])

# Create dataset
dataset_name = "hf-triviaqa-demo"
ls_dataset = client.create_dataset(
    dataset_name=dataset_name,
    description="TriviaQA subset for evaluation"
)
# Push examples to LangSmith
for row in dataset:
    question = row["question"]
    answer = row["answer"]["value"] if isinstance(row["answer"], dict) else row["answer"]
    client.create_example(
        inputs={"input": question},      # CHANGE: use "input" instead of "query"
        outputs={"output": answer},      # CHANGE: use "output" instead of "answer"/"result"
        dataset_id=ls_dataset.id,
    )
print(f"Dataset '{dataset_name}' created with {len(dataset)} examples.")

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Create LLM and prompt
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
prompt = ChatPromptTemplate.from_template(
    "Answer the following question briefly:\n\n{input}"
)

# Modern LangChain approach using LCEL (LangChain Expression Language)
chain = prompt | llm | StrOutputParser()

result = chain.invoke({"input": "What is the capital of France?"})
print(result)

import re  # Import the regular expressions module for pattern matching and text manipulation
import string  # Import string module (though not used directly here, often used for string constants)

# Helper function to extract answer text from a run object
def get_answer_text(run):
    """Extract answer text from run outputs"""

    # Check if the run.outputs object has a 'content' attribute
    if hasattr(run.outputs, 'content'):
        return run.outputs.content  # Return the content directly if present

    # If run.outputs is a dictionary
    elif isinstance(run.outputs, dict):
        # Try to get the value for key "output", if not found, fallback to converting the whole dict to string
        return run.outputs.get("output", "") or str(run.outputs)

    # For any other type of run.outputs
    else:
        return str(run.outputs)  # Convert the outputs to string and return

# Function to normalize text for comparison purposes
def normalize_text(text):
    """Normalize text for comparison"""

    # Convert text to lowercase and remove leading/trailing whitespace
    text = text.lower().strip()

    # Remove all punctuation characters using a regular expression
    text = re.sub(r'[^\w\s]', '', text)  # Keep only word characters and whitespace

    # Replace multiple consecutive whitespace characters with a single space
    text = re.sub(r'\s+', ' ', text)

    # Return the cleaned and normalized text
    return text

# 1. ContainsNumber Evaluator
# This function is essentially an evaluator: it assigns a score based on whether the answer contains any numbers.
def contains_number(run, example):
    """Check if the answer contains any numbers"""

    # Extract the textual answer from the run object using the helper function
    answer_text = get_answer_text(run)

    # Check if any character in the answer text is a digit
    if any(char.isdigit() for char in answer_text):
        # If at least one digit is found, return score 1 and a comment
        return {"score": 1, "comment": "Answer contains a number"}

    # If no digits are found, return score 0 and a comment
    return {"score": 0, "comment": "No number found"}

# Import necessary classes/functions from LangSmith module
from langsmith import Client
from langsmith.evaluation import evaluate

# Informative print statement to indicate the evaluation has started
print("Running evaluation with only one evaluator - Contains number")

# Run the evaluation on a dataset
results = evaluate(
    lambda inputs: chain.invoke(inputs),  # Function that takes inputs and returns outputs
    data=dataset_name,                     # Name of the dataset to evaluate
    evaluators=[contains_number],          # List of evaluator functions to run on each example
    experiment_prefix="triviaqa-eval-contains-number-2"  # Optional project name for LangSmith dashboard
)

# Informative print statement to indicate the evaluation has completed
print("âœ… Evaluation finished!")

# Guide the user to view results in the LangSmith dashboard
print("\nðŸ”— View results in LangSmith by visiting your project dashboard")

# 2. Correctness Evaluator
# This evaluator uses a hierarchy of checks to measure correctness:
# - Exact match
# - Reference fully contained in answer
# - Answer fully contained in reference
# - Word overlap
# - General string similarity
# - It returns a score between 0 and 1 with a descriptive comment.

# Import SequenceMatcher from difflib to compute string similarity
from difflib import SequenceMatcher

# Evaluator to check correctness of the answer against a reference
def correctness_evaluator(run, example):
    """Evaluate if the answer is correct based on reference answer"""

    # Extract the answer text from the run and normalize it
    answer_text = normalize_text(get_answer_text(run))

    # Get the reference output from the example and normalize it
    reference = normalize_text(example.outputs.get("output", ""))

    # If either the reference or the answer is empty, return score 0
    if not reference or not answer_text:
        return {"score": 0, "comment": "Empty answer or reference"}

    # Method 1: Exact match after normalization
    if answer_text == reference:
        return {"score": 1, "comment": "Exact match"}

    # Method 2: Reference string is fully contained within the answer
    if reference in answer_text:
        return {"score": 0.8, "comment": "Reference contained in answer"}

    # Method 3: Answer string is fully contained within the reference
    if answer_text in reference:
        return {"score": 0.7, "comment": "Answer contained in reference"}

    # Method 4: Word-level overlap between reference and answer
    ref_words = set(reference.split())  # Split reference into unique words
    ans_words = set(answer_text.split())  # Split answer into unique words
    common_words = ref_words.intersection(ans_words)  # Find common words

    if len(common_words) > 0:
        # Compute overlap ratio relative to the larger of reference or answer
        overlap_ratio = len(common_words) / max(len(ref_words), len(ans_words))

        # Assign score based on degree of word overlap
        if overlap_ratio >= 0.5:
            return {"score": 0.6, "comment": f"High word overlap ({overlap_ratio:.2f})"}
        elif overlap_ratio >= 0.2:
            return {"score": 0.3, "comment": f"Moderate word overlap ({overlap_ratio:.2f})"}

    # Method 5: Compute general string similarity using SequenceMatcher
    similarity = SequenceMatcher(None, answer_text, reference).ratio()
    if similarity >= 0.6:
        return {"score": 0.4, "comment": f"High similarity ({similarity:.2f})"}

    # If none of the above methods indicate a match, return score 0
    return {"score": 0, "comment": "No meaningful match found"}

# 3. Answer Length Evaluator (Conciseness)
# This evaluator measures answer conciseness by counting words and giving higher scores to shorter answers.
# Itâ€™s useful to balance correctness with readability.
def answer_length_evaluator(run, example):
    """Evaluate answer length - prefer concise but complete answers"""

    # Extract the textual answer from the run object
    answer_text = get_answer_text(run)

    # Count the number of words in the answer
    word_count = len(answer_text.split())

    # Assign score based on word count to encourage conciseness
    if word_count <= 10:
        # Very concise answers (ideal)
        return {"score": 1, "comment": f"Very concise ({word_count} words)"}
    elif word_count <= 25:
        # Appropriately concise answers
        return {"score": 0.8, "comment": f"Appropriately concise ({word_count} words)"}
    elif word_count <= 50:
        # Somewhat verbose answers
        return {"score": 0.6, "comment": f"Somewhat verbose ({word_count} words)"}
    else:
        # Answers longer than 50 words are considered too verbose
        return {"score": 0.2, "comment": f"Too verbose ({word_count} words)"}

# 4. Answer Quality Evaluator
# This evaluator checks for answer quality based on:
# - Empty responses
# - Non-committal language
# - Complete sentences vs. fragments
# - Presence of definitive words for clarity
def answer_quality_evaluator(run, example):
    """Evaluate overall answer quality"""

    # Extract the answer text and remove leading/trailing whitespace
    answer_text = get_answer_text(run).strip()

    # If the answer is empty, return score 0
    if not answer_text:
        return {"score": 0, "comment": "Empty answer"}

    # List of non-committal phrases that indicate low-quality answers
    non_committal = ["i don't know", "not sure", "unclear", "cannot determine",
                     "unable to answer", "no information", "don't have", "can't say"]

    # If any non-committal phrase is present in the answer, give low score
    if any(phrase in answer_text.lower() for phrase in non_committal):
        return {"score": 0.2, "comment": "Non-committal answer"}

    # Check if the answer ends with proper sentence punctuation
    if not answer_text.endswith(('.', '!', '?')):
        # If answer is a single word, it might still be correct
        if len(answer_text.split()) == 1:
            return {"score": 0.8, "comment": "Single word answer (may be correct)"}
        else:
            # Multi-word answer without punctuation considered incomplete
            return {"score": 0.6, "comment": "Incomplete sentence"}

    # Check for presence of common words indicating a specific, definitive answer
    if any(word in answer_text.lower() for word in ['the', 'is', 'was', 'are', 'were']):
        return {"score": 1, "comment": "Definitive, well-formed answer"}

    # Default case for reasonable, but not perfect, answer quality
    return {"score": 0.8, "comment": "Reasonable answer quality"}

# 5. Factual Consistency Evaluator
# This evaluator checks factual consistency by:
# - Detecting the question type (time, location, person, quantity).
# - Ensuring the answer contains relevant indicators (dates, proper names, numbers).
# - Penalizing contradictory or unclear language.
# - Returning a score between 0 and 1 with a descriptive comment listing detected issues.
def factual_consistency_evaluator(run, example):
    """Check if answer is consistent with the question type"""

    # Extract the question text from the example and convert to lowercase
    question = example.inputs.get("input", "").lower()

    # Extract the answer text from the run and convert to lowercase
    answer_text = get_answer_text(run).lower()

    # Initialize score to 1 (perfect consistency) and an empty list for issues
    score = 1.0
    issues = []

    # Question type detection and corresponding consistency checks
    if any(word in question for word in ['when', 'year', 'date']):
        # Time-related question
        if not re.search(r'\b\d{3,4}\b|century|bc|ad|decade', answer_text):
            score -= 0.3
            issues.append("time question but no temporal indicator")

    elif any(word in question for word in ['where', 'which country', 'which state']):
        # Location question
        if len(answer_text.split()) == 1:
            score += 0.1  # Single-word location answers are often good

    elif any(word in question for word in ['who', 'which person']):
        # Person question
        # Check if the answer has a proper name format (First Last)
        if not re.search(r'\b[A-Z][a-z]+ [A-Z][a-z]+\b', get_answer_text(run)):
            score -= 0.2
            issues.append("person question but no proper name format")

    elif any(word in question for word in ['how many', 'how much', 'number']):
        # Quantity question
        if not any(char.isdigit() for char in answer_text) and not any(
            word in answer_text for word in ['zero', 'one', 'two', 'three', 'four', 'five',
                                            'six', 'seven', 'eight', 'nine', 'ten']
        ):
            score -= 0.4
            issues.append("quantity question but no number")

    # Check for contradictory language in the answer
    if "but" in answer_text or "however" in answer_text:
        score -= 0.1
        issues.append("contains contradictory language")

    # Ensure final score is between 0 and 1
    score = max(0, min(1, score))

    # Build a comment string summarizing score and issues
    comment = f"Consistency score: {score:.2f}"
    if issues:
        comment += f" (Issues: {', '.join(issues)})"
    else:
        comment += " (No consistency issues)"

    # Return the score and comment
    return {"score": score, "comment": comment}

# 6. LLM-as-a-Judge Evaluator
# This evaluator uses GPT-4 as an expert judge to assess answers based on:
# - Correctness
# - Completeness
# - Clarity
# - Relevance
def llm_judge_evaluator(run, example):
    """Use GPT-4 as a judge to evaluate answer quality and correctness"""

    try:
        # Initialize GPT-4 LLM with zero temperature for deterministic evaluation
        judge_llm = ChatOpenAI(model="gpt-4", temperature=0)

        # Extract question, model answer, and reference answer
        question = example.inputs.get("input", "")
        answer = get_answer_text(run)
        reference = example.outputs.get("output", "")

        # If the answer is empty, return score 0 immediately
        if not answer.strip():
            return {"score": 0, "comment": "Empty answer"}

        # Build evaluation prompt for GPT-4
        evaluation_prompt = f"""You are an expert evaluator. Please evaluate the following question-answer pair.

Question: {question}
Reference Answer: {reference}
Model Answer: {answer}

Please evaluate the model answer on a scale of 0-1 based on:
1. Correctness: Is the answer factually correct?
2. Completeness: Does it fully address the question?
3. Clarity: Is it clear and well-expressed?
4. Relevance: Is it relevant to the question asked?

Provide your evaluation in this exact format:
SCORE: [number between 0 and 1]
REASONING: [brief explanation of your scoring]

Be strict but fair. A score of 1.0 should be reserved for perfect answers."""

        try:
            # Call GPT-4 with the evaluation prompt
            response = judge_llm.invoke(evaluation_prompt)
            evaluation_text = response.content

            # Parse the SCORE and REASONING from GPT-4 response using regex
            import re
            score_match = re.search(r'SCORE:\s*([0-9]*\.?[0-9]+)', evaluation_text)
            reasoning_match = re.search(r'REASONING:\s*(.+)', evaluation_text, re.DOTALL)

            if score_match:
                score = float(score_match.group(1))
                score = max(0, min(1, score))  # Clamp score between 0 and 1
            else:
                score = 0.5  # Default if parsing fails

            if reasoning_match:
                reasoning = reasoning_match.group(1).strip()[:200]  # Truncate reasoning if too long
            else:
                reasoning = "LLM evaluation completed"

            return {"score": score, "comment": f"LLM Judge: {reasoning}"}

        except Exception as e:
            # Fallback: use simple normalized string similarity if LLM call fails
            answer_norm = normalize_text(answer)
            ref_norm = normalize_text(reference)

            if ref_norm in answer_norm or answer_norm in ref_norm:
                return {"score": 0.7, "comment": "LLM unavailable, using similarity fallback"}
            else:
                return {"score": 0.3, "comment": "LLM unavailable, no clear match"}

    except Exception as e:
        # Return error score if initialization or other critical failure occurs
        return {"score": 0, "comment": f"LLM Judge error: {str(e)[:100]}"}

# 7. Response Completeness Evaluator
# This evaluator measures whether the answer fully addresses the question by:
# - Extracting meaningful words from question and answer.
# - Ignoring stop words for relevance.
# - Computing word overlap ratio between question and answer.
# - Assigning a score based on overlap, with adjustments for empty answers.
def response_completeness_evaluator(run, example):
    """Evaluate if the response fully addresses the question"""

    # Extract question and convert to lowercase
    question = example.inputs.get("input", "").lower()

    # Extract answer text
    answer_text = get_answer_text(run)

    # Split question and answer into sets of words
    question_words = set(re.findall(r'\b\w+\b', question))
    answer_words = set(re.findall(r'\b\w+\b', answer_text.lower()))

    # Define common stop words to ignore for meaningful word comparison
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',
                  'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were',
                  'what', 'which', 'who', 'when', 'where', 'why', 'how'}

    # Remove stop words to get meaningful words from question and answer
    meaningful_q_words = question_words - stop_words
    meaningful_a_words = answer_words - stop_words

    # If there are no meaningful words in the question, assign neutral score
    if not meaningful_q_words:
        return {"score": 0.5, "comment": "Cannot analyze question context"}

    # Calculate overlap between meaningful words in question and answer
    word_overlap = len(meaningful_q_words.intersection(meaningful_a_words))
    overlap_ratio = word_overlap / len(meaningful_q_words)

    # Assign score based on the degree of overlap
    if overlap_ratio >= 0.3:
        return {"score": 1, "comment": f"Good context overlap ({overlap_ratio:.2f})"}
    elif overlap_ratio >= 0.1:
        return {"score": 0.7, "comment": f"Moderate context overlap ({overlap_ratio:.2f})"}
    else:
        # Even with low word overlap, if answer is non-empty, assign moderate score
        if len(answer_text.strip()) > 0:
            return {"score": 0.5, "comment": "Low context overlap but provides answer"}
        else:
            # Empty answer gets score 0
            return {"score": 0, "comment": "No meaningful response"}

# Import necessary classes/functions from LangSmith module
from langsmith import Client
from langsmith.evaluation import evaluate

# Create evaluation configuration with all 7 custom evaluators
evaluators=[
    contains_number,                 # Checks if answer contains numbers
    correctness_evaluator,           # Checks factual correctness
    answer_length_evaluator,         # Evaluates conciseness
    answer_quality_evaluator,        # Evaluates overall answer quality
    factual_consistency_evaluator,   # Checks consistency with question type
    llm_judge_evaluator,             # Uses GPT-4 to judge correctness & quality
    response_completeness_evaluator  # Checks if answer addresses question fully
]

# Inform the user which evaluators are being run
print("Running evaluation with 7 custom evaluators:")
print("1. Contains Number")
print("2. Correctness")
print("3. Answer Length (Conciseness)")
print("4. Answer Quality")
print("5. Factual Consistency")
print("6. LLM-as-a-Judge (GPT-4)")
print("7. Response Completeness")

# Run the evaluation on the dataset
results = evaluate(
    lambda inputs: chain.invoke(inputs),  # Function that takes inputs and returns outputs
    data=dataset_name,                    # Dataset to evaluate
    evaluators=evaluators,                # List of evaluator functions
    experiment_prefix="triviaqa-eval-comprehensive-eval"  # Project name for dashboard
)

# Indicate completion of evaluation
print("âœ… Evaluation finished!")

# Print summary statistics if results are present
print(f"\nEvaluated examples with 7 custom evaluators")

# Guide the user to view results in LangSmith dashboard
print("\nðŸ”— View results in LangSmith by visiting your project dashboard")